{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f43061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a49a5",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897cb01",
   "metadata": {},
   "source": [
    "$$ \\mathbf{C(w)}=  \\sum\\limits_{i=1}^{m}  (y^{i}- \\mathbf{w^{T}x^{i}})^2 $$\n",
    "\n",
    "Where: \n",
    "$\\mathbf{x^{i}} = [1, x_1^{i}, x_2^{i}...x_n^{i}]$, \n",
    "$\\mathbf{w^{i}} = [w_0, w_1, w_2...w_n]$\n",
    "\n",
    "The gradient is:\n",
    "$$  \\mathbf{\\nabla C(w)} = -\\mathbf{X^{T}}(y-\\mathbf{Xw})  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class linear_regression():\n",
    "    def __init__(self, learning_rate=0.01, iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # add constant to X: n*(k+1)\n",
    "        self.observations = len(X)\n",
    "        self.dimentions = len(X[0])+1\n",
    "        X = np.c_[X, np.ones((self.observations,1))]    \n",
    "        \n",
    "        #initalize weight: k+1 * 1\n",
    "        weights = np.ones((self.dimentions, 1))\n",
    "        \n",
    "        for _ in self.iterations:\n",
    "            # compute errors: n*1\n",
    "            errors = y - np.dot(X, weights) \n",
    "            # compute gradient: k+1 * 1\n",
    "            gradient = -X.T * errors\n",
    "            # update weights\n",
    "            weight -= self.learning_rate * gradient\n",
    "        \n",
    "        self.weights = weights\n",
    "        return self.weight\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X.dot(self.weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbd949",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60108138",
   "metadata": {},
   "source": [
    "$$ J(\\theta)=-\\frac{1}{m} \\sum\\limits_{i=1}^{m} [y^i log( h_{\\theta}(x^i)) + (1-y^i) log(1-h_{\\theta}(x^i))]   $$\n",
    "\n",
    "Where: $h(x) = \\dfrac{1}{1+e^{-\\theta^T x}}$, note $h^{'}(x) = h(x)(1-h(x))$\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$  \\nabla J(\\theta) =  -X^T(y^i - h_{\\theta}(x^i)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888115fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class logistic_regression():\n",
    "    def __init__(self, learning_rate, iteration):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = iteration\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def _entropy(self, theta, X, y):\n",
    "        loss0 = -np.dot(y, np.log(self._sigmoid(X.dot(theta))))\n",
    "        loss1 = -np.dot(1-y, np.log(1- self._sigmoid(X.dot(theta))))\n",
    "        return loss0 + loss1\n",
    "    \n",
    "    def _scale(self, X):\n",
    "        for i in range(len(a[0])):\n",
    "            a[:, i] = (a[:, i] - min(a[:, i]))/ np.ptp(a[:, i])\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        observations, dimensions = X.shape\n",
    "        # initalize theta\n",
    "        theta = np.zeros((dimensions, 1))  # (k+1)*1\n",
    "        \n",
    "        for _ in self.iteration:\n",
    "            # computer error\n",
    "            loss = self._entropy(theta, X, y)\n",
    "            # computer gradient\n",
    "            gradient = -X.T*(y- self._sigmoid(np.dot(X, theta)))\n",
    "            # update theta\n",
    "            theta -= self.learning_rate*gradient\n",
    "        \n",
    "        self.theta = theta\n",
    "        return self.theta\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_hat = self._sigmoid(np.dot(X, self.theta))\n",
    "        return 1 if y_hat >= 1/2 else 0\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e311ce5",
   "metadata": {},
   "source": [
    "## KNN\n",
    "\n",
    "Time complexity: O(N\\*K)+O(Nlog(N))\n",
    "\n",
    "Space complexity: O(N)\n",
    "\n",
    "How to find the optimal K:\n",
    "- simple approach: $\\sqrt{K}$\n",
    "- CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2103a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class KNN():\n",
    "    def ___inite__(self, X=None, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def _dist(self, x1, x2):\n",
    "            diff = x1 - x2\n",
    "            return np.dot(diff.T, diff)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    \n",
    "    def preict(self, x, k):\n",
    "        distance_label = [(self._dist(x, point), label) for (point, label) in zip(self.X, self.y)]\n",
    "        neighbors = sorted(distance_label, key=lambda tup: tup[0])[:k]\n",
    "            \n",
    "        return Counter([label for _, label in neighbors]).most_common()[0][0]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61621234",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db805d",
   "metadata": {},
   "source": [
    "Pesudocode:\n",
    "1. intializz centroids\n",
    "\n",
    "2. repeat until centroids remain same\n",
    "\n",
    "     - compute distance of points to every centroid\n",
    "     \n",
    "     - label the point to nearest one\n",
    "     \n",
    "     - compute new centroids\n",
    "\n",
    "\n",
    "Time complexity: O(N\\*K\\*I)\n",
    "\n",
    "Space complexity: O(N+K)/ O(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e961d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# 可以找空间内的任意点，而不是数据集中的点\n",
    "def initial_centroids(data, k, seed=None):\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    return random.sample(data, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2696ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def dist(point1, point2):\n",
    "    return (point1[0]-point2[0])^2+(point1[1]-point2[1])^2\n",
    "\n",
    "def compute_and_label(data, centroids):\n",
    "    labels = [-1]*len(data)\n",
    "    \n",
    "    for i, point in enumerate(data):\n",
    "        min_dist = float(math.inf)\n",
    "        \n",
    "        for j, centr in enumerate(centroids):\n",
    "            if dist(point, centr) <= min_dist:\n",
    "                labels[i] = j\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b274e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中心点 不一定要是 数据点！！\n",
    "def update_centroid(data, labels, k):\n",
    "    new_centroid = [(0,0)]*k\n",
    "    count = [0]*k\n",
    "\n",
    "    for (point, label) in zip(data, labels):\n",
    "        count[label] += 1\n",
    "        new_centroid[label][0] += point[0]\n",
    "        new_centroid[label][1] += point[1]\n",
    "    \n",
    "    for idx, (x,y) in enumerate(new_centroid):\n",
    "        centr[0] = x/count[idx]\n",
    "        centr[1] = y/count[idx]\n",
    "        \n",
    "    return new_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10011c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def should_stop(old, new, threshold=1e-5):\n",
    "    total_movement = 0\n",
    "    \n",
    "    for (p1, p2) in zip(old, new):\n",
    "        total_movement += math.sqrt(p1, p2)\n",
    "    \n",
    "    return total_movement <= threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c8f5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data, k, seed=None, threshold=1e-5):\n",
    "    old_centroids = []\n",
    "    new_centroids = initial_centroids(data, k, seed=seed)\n",
    "    \n",
    "    while True:\n",
    "        labels = compute_and_label(data, new_centroid)\n",
    "        old_centroid = new_centroid\n",
    "        new_centroid = update_centroid(data, labels)\n",
    "        \n",
    "        if should_stop(old_centroid, new_centroid, threshold):\n",
    "            break\n",
    "            \n",
    "    return labels  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbdf2bb",
   "metadata": {},
   "source": [
    "## [Decision Tree](https://towardsdatascience.com/implementing-a-decision-tree-from-scratch-f5358ff9c4bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe22f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d641a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=100, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_samples_split\n",
    "        self.root = None\n",
    "        \n",
    "    \n",
    "    def _is_finished(self, depth):\n",
    "        if (depth > self.max_depth \n",
    "            or self.n_class_labels == 1\n",
    "            or self.n_samples < self.min_sample_split):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        proportions = np.bincount(y)/len(y)\n",
    "        entropy =  np.sum([ p*np.log2(p) for p in proportions if p >0])\n",
    "\n",
    "\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        left_idx = np.argwhere(X<=thresh).flatten\n",
    "        right_idx = np.argwhere(X>thresh).flatten\n",
    "        return left_idx, right_idx\n",
    "\n",
    "\n",
    "\n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        parent_loss = self._entropy(y)\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "        \n",
    "        if n_left ==0 or n_right == 0:\n",
    "            return 0\n",
    "        \n",
    "        child_loss = (n_left)/n*self._entropy(y[left_idx])  + (n_right)/n*self._entropy(y[right_idx])\n",
    "        \n",
    "\n",
    "\n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {\"score\": -1, \"feature\": None, \"thresh\": None}\n",
    "        \n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "                \n",
    "                if score > split(\"score\"):\n",
    "                    split[\"score\"] = score\n",
    "                    split[\"feat\"] = feat\n",
    "                    split[\"thresh\"] = thresh\n",
    "        \n",
    "        return split[\"feat\"], split[\"thresh\"]\n",
    "            \n",
    " \n",
    "        \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.n_class_labels = len(np.unique(y))\n",
    "        \n",
    "        #stopping critieria\n",
    "        if self._is_finished(depth):\n",
    "            most_common_label = np.argmax(np.bincount(y))\n",
    "            return Node(value=most_common_label)\n",
    "        \n",
    "        #get best split\n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "        best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "        \n",
    "        #grow children recursively\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth+1)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth+1)\n",
    "        \n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "     \n",
    "    \n",
    "     \n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "68adcd4174e0681d222f6db10e0e7b881828c252a2e3da061512940792d75315"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
